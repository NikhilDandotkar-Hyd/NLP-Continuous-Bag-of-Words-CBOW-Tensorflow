{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Continuous Bag of Words (CBOW) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CBOW model architecture tries to predict the current target word (the center word) based on the source context words (surrounding words). \n",
    "\n",
    "Considering a simple sentence, “the quick brown fox jumps over the lazy dog”, this can be pairs of (context_window, target_word) where if we consider a context window of size 2, we have examples like ([quick, fox], brown), ([the, brown], quick), ([the, dog], lazy) and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the step by step method for implementation:\n",
    "\n",
    "**1. Build the corpus vocabulary**\n",
    "\n",
    "**2. Build a CBOW (context, target) generator**\n",
    "\n",
    "**3. Build the CBOW model architecture**\n",
    "\n",
    "**4. Train the Model** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary packages\n",
    "\n",
    "word2vec_utils is the utility file which contains functions such as downloading data,building vocabulary,building sample generator etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils\n",
    "import word2vec_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url = 'http://mattmahoney.net/dc/text8.zip'\n",
    "local_dest = 'data/text8.zip'\n",
    "expected_byte = 31344016\n",
    "vocab_size = 50000\n",
    "visual_fld = 'visualization'\n",
    "window_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1 : Build the corpus vocabulary\n",
    "Download the corpus and convert the corpus data to list of individual words\n",
    "\n",
    "and then all the unique words in the list are collected and assigned with the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/text8.zip already exists\n"
     ]
    }
   ],
   "source": [
    "utils.download_one_file(download_url, local_dest, expected_byte)\n",
    "words = word2vec_utils.read_data(local_dest)\n",
    "dictionary, _ = word2vec_utils.build_vocab(words, vocab_size, visual_fld)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Build a CBOW (context, target) generator\n",
    "Here each word in the corpus is assigned with index from the dictionary that was created in the previous step\n",
    "\n",
    "We need pairs which consist of a target centre word and surround context words. In our implementation, a target word is of length 1 and surrounding context is of length 2 x window_size where we take window_size words before and after the target word in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object generate_cbow_sample at 0x0000013F8A360A20>\n"
     ]
    }
   ],
   "source": [
    "index_words = word2vec_utils.convert_words_to_index(words, dictionary)\n",
    "train = word2vec_utils.generate_cbow_sample(index_words,window_size)\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of pairs of context words and target word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ sample  1  ------\n",
      "X:context -> ['so', 'soviet', 'abuse', 'be']\n",
      "Y:target -> class\n",
      "------ sample  2  ------\n",
      "X:context -> ['soviet', 'class', 'be', 'as']\n",
      "Y:target -> abuse\n",
      "------ sample  3  ------\n",
      "X:context -> ['class', 'abuse', 'as', 'the']\n",
      "Y:target -> be\n",
      "------ sample  4  ------\n",
      "X:context -> ['abuse', 'be', 'the', 'means']\n",
      "Y:target -> as\n",
      "------ sample  5  ------\n",
      "X:context -> ['be', 'as', 'means', 'as']\n",
      "Y:target -> the\n",
      "------ sample  6  ------\n",
      "X:context -> ['as', 'the', 'as', 'economic']\n",
      "Y:target -> means\n",
      "------ sample  7  ------\n",
      "X:context -> ['the', 'means', 'economic', 'use']\n",
      "Y:target -> as\n",
      "------ sample  8  ------\n",
      "X:context -> ['means', 'as', 'use', 'this']\n",
      "Y:target -> economic\n",
      "------ sample  9  ------\n",
      "X:context -> ['as', 'economic', 'this', 'the']\n",
      "Y:target -> use\n",
      "------ sample  10  ------\n",
      "X:context -> ['economic', 'use', 'the', 'least']\n",
      "Y:target -> this\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "for x,y in train:\n",
    "    if iteration >= 10:\n",
    "        break\n",
    "    print('------ sample ',iteration+1,' ------')\n",
    "    print('X:context ->',[words[i] for i in x])\n",
    "    print('Y:target ->',words[y])\n",
    "    iteration +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Build the CBOW model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cbow_batch_gen** generator is used to make batches of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cbow_batch_gen(batch_size,train_new):\n",
    "    while True:\n",
    "        context_batch = np.zeros([batch_size,2*window_size], dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, 1])\n",
    "        for i in range(batch_size):\n",
    "            context_batch[i],target_batch[i] = next(train_new)\n",
    "        yield context_batch,target_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data in batches with the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "WINDOW_SIZE = 2\n",
    "VOCAB_SIZE = 50000\n",
    "EMBED_SIZE = 100\n",
    "train = word2vec_utils.generate_cbow_sample(index_words,window_size)\n",
    "def gen():\n",
    "    yield from cbow_batch_gen(BATCH_SIZE,train)\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(gen, \n",
    "                                (tf.int32, tf.int32), \n",
    "                                (tf.TensorShape([BATCH_SIZE,2*WINDOW_SIZE]), tf.TensorShape([BATCH_SIZE, 1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('data'):\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        context_words, target_word = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating Embedding matix where each word is embedded with EMBED_SIZE length vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('embed'):\n",
    "        embed_matrix = tf.get_variable('embed_matrix1', \n",
    "                                        shape=[VOCAB_SIZE, EMBED_SIZE],\n",
    "                                        initializer=tf.random_uniform_initializer())\n",
    "        embed = tf.nn.embedding_lookup(embed_matrix, context_words, name='embedding1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "taking mean of context embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(iterator.initializer)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "embed_mean = tf.reduce_mean(embed,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning weights and bias for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "        nce_weight = tf.get_variable('nce_weight', shape=[VOCAB_SIZE, EMBED_SIZE],\n",
    "                        initializer=tf.truncated_normal_initializer(stddev=1.0 / (EMBED_SIZE ** 0.5)))\n",
    "        nce_bias = tf.get_variable('nce_bias', initializer=tf.zeros([VOCAB_SIZE]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NCE loss fuction is used for estimating the loss \n",
    "\n",
    "http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0715 21:29:22.214289  4068 deprecation.py:323] From c:\\users\\nikhi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "NUM_SAMPLED = 64            \n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n",
    "                                     biases=nce_bias, \n",
    "                                     labels=target_word, \n",
    "                                     inputs=embed_mean, \n",
    "                                     num_sampled=NUM_SAMPLED, \n",
    "                                     num_classes=VOCAB_SIZE), name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient descent optimizer is used for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.5\n",
    "with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: Traning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4999:  63.3\n",
      "Average loss at step 9999:  17.4\n",
      "Average loss at step 14999:   9.1\n",
      "Average loss at step 19999:   6.3\n",
      "Average loss at step 24999:   5.1\n",
      "Average loss at step 29999:   4.8\n",
      "Average loss at step 34999:   4.6\n",
      "Average loss at step 39999:   4.5\n",
      "Average loss at step 44999:   4.4\n",
      "Average loss at step 49999:   4.4\n",
      "Average loss at step 54999:   4.4\n",
      "Average loss at step 59999:   4.4\n",
      "Average loss at step 64999:   4.4\n",
      "Average loss at step 69999:   4.4\n",
      "Average loss at step 74999:   4.3\n",
      "Average loss at step 79999:   4.3\n",
      "Average loss at step 84999:   4.3\n",
      "Average loss at step 89999:   4.3\n",
      "Average loss at step 94999:   4.3\n",
      "Average loss at step 99999:   4.3\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN_STEPS = 100000\n",
    "SKIP_STEP = 5000\n",
    "with tf.Session() as sess:\n",
    "        sess.run(iterator.initializer)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        total_loss = 0.0 # we use this to calculate late average loss in the last SKIP_STEP steps\n",
    "        writer = tf.summary.FileWriter('graphs/cbow_word2vec', sess.graph)\n",
    "\n",
    "        for index in range(NUM_TRAIN_STEPS):\n",
    "            try:\n",
    "                loss_batch, _ = sess.run([loss, optimizer])\n",
    "                total_loss += loss_batch\n",
    "                if (index + 1) % SKIP_STEP == 0:\n",
    "                    print('Average loss at step {}: {:5.1f}'.format(index, total_loss / SKIP_STEP))\n",
    "                    total_loss = 0.0\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                sess.run(iterator.initializer)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
